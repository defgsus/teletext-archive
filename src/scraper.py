import os
import sys
import glob
from pathlib import Path
from typing import Generator, Tuple, Union, Optional

import requests
import bs4

scraper_classes = dict()


class Scraper:

    # must be filename compatible
    NAME: str = None
    # set to True in abstract classes
    ABSTRACT: bool = False

    NUM_PAGE_DIGITS: int = 3
    NUM_SUB_PAGE_DIGITS: int = 2
    FILE_EXTENSION: str = "html"

    # request timeout in seconds
    REQUEST_TIMEOUT: float = 5

    BASE_PATH: Path = Path(__file__).resolve().parent.parent / "docs" / "snapshots"

    def __init_subclass__(cls, **kwargs):
        if not cls.ABSTRACT:
            assert cls.NAME, f"Define {cls.__name__}.NAME"

            if cls.NAME in scraper_classes:
                raise AssertionError(f"Duplicate name '{cls.NAME}' for class {cls.__name__}")

            scraper_classes[cls.NAME] = cls

    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.session = requests.Session()
        self.session.headers = {
            "User-Agent": "github.com/defgsus/teletext-archive"
        }

    @classmethod
    def path(cls) -> Path:
        return cls.BASE_PATH / cls.NAME

    @classmethod
    def to_filename(cls, page_index: int, sub_page_index: int) -> Path:
        return cls.path() / (
            f"{page_index:0{cls.NUM_PAGE_DIGITS}}"
            f"-{sub_page_index:0{cls.NUM_SUB_PAGE_DIGITS}}.{cls.FILE_EXTENSION}"
        )

    def iter_pages(self) -> Generator[Tuple[int, int, Union[str, bool]], None, None]:
        """
        Yield tuples of (page-number, sub-page-number, html-text)

        In cases that the content does not have changed except
        for the autogenerated date/time display in the teletext,
        one can yield True instead of the text to keep the file.
        See ZDF scraper as example.
        """
        raise NotImplementedError

    def download(self) -> dict:
        """
        Download all pages via `iter_pages` and store to disk

        Returns a small report dict.
        """
        report = {
            "unchanged": 0,
            "changed": 0,
            "added": 0,
            "removed": 0,
        }
        received_files = set()

        if not self.path().exists():
            os.makedirs(str(self.path()))
            existing_files = set()
        else:
            existing_files = {
                Path(fn)
                for fn in glob.glob(str(self.path() / f"*.{self.FILE_EXTENSION}"))
            }

        for page_num, sub_page_num, content in self.iter_pages():

            filename = self.to_filename(page_num, sub_page_num)
            received_files.add(filename)

            if content is True:
                report["unchanged"] += 1
                continue

            if filename.exists():
                previous_content = filename.read_text()
                if previous_content == content:
                    report["unchanged"] += 1
                    continue
                else:
                    report["changed"] += 1

            self.log("storing", filename)
            filename.write_text(content)

        removed_files = existing_files - received_files
        report["added"] = len(received_files - existing_files)
        report["removed"] = len(removed_files)

        for fn in removed_files:
            self.log("deleting", fn)
            os.remove(str(fn))

        return report

    def log(self, *args):
        if self.verbose:
            print(f"{self.__class__.__name__}:", *args, file=sys.stderr)

    def get_html(self, url: str, method: str = "GET", **kwargs) -> requests.Response:
        kwargs.setdefault("timeout", self.REQUEST_TIMEOUT)
        self.log("requesting", url)
        return self.session.request(method=method, url=url, **kwargs)

    def get_soup(
            self,
            url: str,
            method: str = "GET",
            expected_status: int = 200,
            **kwargs
    ) -> Optional[bs4.BeautifulSoup]:
        response = self.get_html(url=url, method=method, **kwargs)
        if response.status_code != expected_status:
            return None
        return bs4.BeautifulSoup(response.text, features="html.parser")
